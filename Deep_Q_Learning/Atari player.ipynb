{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aman/Documents/conda_gym/env_gym/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, step=0, info=\"\"):\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"Atari Breakout\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Reset it, returns the starting frame\n",
    "frame = env.reset()\n",
    "\n",
    "\n",
    "is_done = False\n",
    "while not is_done:\n",
    "    # Perform a random action, returns the new frame, reward and whether the game is over\n",
    "    frame, reward, is_done, _ = env.step(env.action_space.sample())\n",
    "    show_state(env.env)\n",
    "    plt.clf()\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_grayscale(img):\n",
    "    return np.mean(img, axis=2).astype(np.uint8)\n",
    "\n",
    "def downsample(img):\n",
    "    return img[::2, ::2]\n",
    "\n",
    "def preprocess(img):\n",
    "    return to_grayscale(downsample(img))\n",
    "\n",
    "def transform_reward(reward):\n",
    "    return np.sign(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_batch(model, gamma, start_states, actions, rewards, next_states, is_terminal):\n",
    "    # First, predict the Q values of the next states. Note how we are passing ones as the mask.\n",
    "    next_Q_values = model.predict([next_states, np.ones(actions.shape)])\n",
    "    # The Q values of the terminal states is 0 by definition, so override them\n",
    "    next_Q_values[is_terminal] = 0\n",
    "    # The Q values of each start state is the reward + gamma * the max next state Q value\n",
    "    Q_values = rewards + gamma * np.max(next_Q_values, axis=1)\n",
    "    # Fit the keras model. Note how we are passing the actions as the mask and multiplying\n",
    "    # the targets by the actions.\n",
    "    model.fit([start_states, actions], actions * Q_values[:, None],nb_epoch=1, batch_size=len(start_states), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atari_model(n_actions):\n",
    "    ATARI_SHAPE = (105, 80, 4)\n",
    "\n",
    "    # With the functional API we need to define the inputs.\n",
    "    frames_input = keras.layers.Input(ATARI_SHAPE, name='frames')\n",
    "    actions_input = keras.layers.Input((n_actions,), name='mask')\n",
    "\n",
    "    # Assuming that the input frames are still encoded from 0 to 255. Transforming to [0, 1].\n",
    "    normalized = keras.layers.Lambda(lambda x: x / 255.0)(frames_input)\n",
    "    \n",
    "    # \"The first hidden layer convolves 16 8×8 filters with stride 4 with the input image and applies a rectifier nonlinearity.\"\n",
    "    conv_1 = keras.layers.Conv2D(16, (8, 8), activation='relu', strides=(4, 4))(normalized)\n",
    "    \n",
    "    # \"The second hidden layer convolves 32 4×4 filters with stride 2, again followed by a rectifier nonlinearity.\"\n",
    "    conv_2 = keras.layers.Conv2D(32, (4, 4), activation=\"relu\", strides=(2, 2))(conv_1)\n",
    "    \n",
    "    # Flattening the second convolutional layer.\n",
    "    conv_flattened = keras.layers.Flatten()(conv_2)\n",
    "    # \"The final hidden layer is fully-connected and consists of 256 rectifier units.\"\n",
    "    hidden = keras.layers.Dense(256, activation='relu')(conv_flattened)\n",
    "    # \"The output layer is a fully-connected linear layer with a single output for each valid action.\"\n",
    "    output = keras.layers.Dense(n_actions)(hidden)\n",
    "    # Finally, we multiply the output by the mask!\n",
    "    filtered_output = keras.layers.merge([output, actions_input], mode='mul')\n",
    "\n",
    "    model = keras.models.Model(input=[frames_input, actions_input], output=filtered_output)\n",
    "    optimizer = optimizer=keras.optimizers.RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)\n",
    "    model.compile(optimizer, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RingBuf:\n",
    "    def __init__(self, size):\n",
    "        # Pro-tip: when implementing a ring buffer, always allocate one extra element,\n",
    "        # this way, self.start == self.end always means the buffer is EMPTY, whereas\n",
    "        # if you allocate exactly the right number of elements, it could also mean\n",
    "        # the buffer is full. This greatly simplifies the rest of the code.\n",
    "        self.data = [None] * (size + 1)\n",
    "        self.start = 0\n",
    "        self.end = 0\n",
    "        \n",
    "    def append(self, element):\n",
    "        self.data[self.end] = element\n",
    "        self.end = (self.end + 1) % len(self.data)\n",
    "        # end == start and yet we just added one element. This means the buffer has one\n",
    "        # too many element. Remove the first element by incrementing start.\n",
    "        if self.end == self.start:\n",
    "            self.start = (self.start + 1) % len(self.data)\n",
    "    \n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        self.append((state, action, next_state, reward, done))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[(self.start + idx) % len(self.data)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.end < self.start:\n",
    "            return self.end + len(self.data) - self.start\n",
    "        else:\n",
    "            return self.end - self.start\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_iteration(env, model, state, iteration, memory):\n",
    "    # Choose epsilon based on the iteration\n",
    "    epsilon = get_epsilon_for_iteration(iteration)\n",
    "\n",
    "    # Choose the action \n",
    "    if random.random() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = choose_best_action(model, state)\n",
    "\n",
    "    # Play one game iteration (note: according to the next paper, you should actually play 4 times here)\n",
    "    new_frame, reward, is_done, _ = env.step(action)\n",
    "    memory.add(state, action, new_frame, reward, is_done)\n",
    "\n",
    "    # Sample and fit\n",
    "    batch = memory.sample_batch(32)\n",
    "    fit_batch(model, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
